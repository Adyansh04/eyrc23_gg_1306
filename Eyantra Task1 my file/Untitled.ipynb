{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc6752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 14.0929\n",
      "Epoch [2/50], Loss: 16.7222\n",
      "Epoch [3/50], Loss: 6.0258\n",
      "Epoch [4/50], Loss: 10.6342\n",
      "Epoch [5/50], Loss: 5.9199\n",
      "Epoch [6/50], Loss: 4.4159\n",
      "Epoch [7/50], Loss: 6.4959\n",
      "Epoch [8/50], Loss: 5.3707\n",
      "Epoch [9/50], Loss: 8.0882\n",
      "Epoch [10/50], Loss: 6.6859\n",
      "Epoch [11/50], Loss: 11.5624\n",
      "Epoch [12/50], Loss: 8.7869\n",
      "Epoch [13/50], Loss: 5.0981\n",
      "Epoch [14/50], Loss: 3.8784\n",
      "Epoch [15/50], Loss: 1.7858\n",
      "Epoch [16/50], Loss: 4.5183\n",
      "Epoch [17/50], Loss: 1.6744\n",
      "Epoch [18/50], Loss: 2.7945\n",
      "Epoch [19/50], Loss: 3.8083\n",
      "Epoch [20/50], Loss: 1.8052\n",
      "Epoch [21/50], Loss: 1.0659\n",
      "Epoch [22/50], Loss: 1.2941\n",
      "Epoch [23/50], Loss: 1.1057\n",
      "Epoch [24/50], Loss: 0.8723\n",
      "Epoch [25/50], Loss: 1.5770\n",
      "Epoch [26/50], Loss: 2.0905\n",
      "Epoch [27/50], Loss: 0.9694\n",
      "Epoch [28/50], Loss: 1.0480\n",
      "Epoch [29/50], Loss: 0.8475\n",
      "Epoch [30/50], Loss: 1.5459\n",
      "Epoch [31/50], Loss: 0.8799\n",
      "Epoch [32/50], Loss: 0.7926\n",
      "Epoch [33/50], Loss: 0.7757\n",
      "Epoch [34/50], Loss: 0.8629\n",
      "Epoch [35/50], Loss: 0.9723\n",
      "Epoch [36/50], Loss: 0.9243\n",
      "Epoch [37/50], Loss: 1.0779\n",
      "Epoch [38/50], Loss: 0.7437\n",
      "Epoch [39/50], Loss: 0.8529\n",
      "Epoch [40/50], Loss: 2.0060\n",
      "Epoch [41/50], Loss: 0.9775\n",
      "Epoch [42/50], Loss: 0.8622\n",
      "Epoch [43/50], Loss: 1.0448\n",
      "Epoch [44/50], Loss: 0.9116\n",
      "Epoch [45/50], Loss: 0.6765\n",
      "Epoch [46/50], Loss: 0.7621\n",
      "Epoch [47/50], Loss: 0.7112\n",
      "Epoch [48/50], Loss: 0.7442\n",
      "Epoch [49/50], Loss: 0.8253\n",
      "Epoch [50/50], Loss: 0.7447\n",
      "Validation Accuracy: 66.24%\n",
      "Accuracy on the test set = 0.6623516720604099\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "*****************************************************************************************\n",
    "*\n",
    "*        \t\t===============================================\n",
    "*           \t\tGeoGuide(GG) Theme (eYRC 2023-24)\n",
    "*        \t\t===============================================\n",
    "*\n",
    "*  This script is to implement Task 1A of GeoGuide(GG) Theme (eYRC 2023-24).\n",
    "*  \n",
    "*  This software is made available on an \"AS IS WHERE IS BASIS\".\n",
    "*  Licensee/end user indemnifies and will keep e-Yantra indemnified from\n",
    "*  any and all claim(s) that emanate from the use of the Software or \n",
    "*  breach of the terms of this agreement.\n",
    "*\n",
    "*****************************************************************************************\n",
    "'''\n",
    "\n",
    "# Team ID:\t\t\tGG_1306\n",
    "# Author List:\t\t['Adyansh Gupta', 'Chetas Hedaoo', 'Harsh Mehta', 'Megha Datta']\n",
    "# Filename:\t\t\ttask_1a.py\n",
    "# Functions:\t    [`ideantify_features_and_targets`, `load_as_tensors`,\n",
    "# \t\t\t\t\t `model_loss_function`, `model_optimizer`, `model_number_of_epochs`, `training_function`,\n",
    "# \t\t\t\t\t `validation_functions` ]\n",
    "\n",
    "####################### IMPORT MODULES #######################\n",
    "import pandas \n",
    "import torch\n",
    "import numpy \n",
    "###################### Additional Imports ####################\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "##############################################################\n",
    "\n",
    "################# ADD UTILITY FUNCTIONS HERE #################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def data_preprocessing(task_1a_dataframe):\n",
    "\n",
    "    ''' \n",
    "    Purpose:\n",
    "    ---\n",
    "    This function will be used to load your csv dataset and preprocess it.\n",
    "    Preprocessing involves cleaning the dataset by removing unwanted features,\n",
    "    decision about what needs to be done with missing values etc. Note that \n",
    "    there are features in the csv file whose values are textual (eg: Industry, \n",
    "    Education Level etc)These features might be required for training the model\n",
    "    but can not be given directly as strings for training. Hence this function \n",
    "    should return encoded dataframe in which all the textual features are \n",
    "    numerically labeled.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    `task_1a_dataframe`: [Dataframe]\n",
    "                        Pandas dataframe read from the provided dataset \t\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `encoded_dataframe` : [ Dataframe ]\n",
    "                        Pandas dataframe that has all the features mapped to \n",
    "                        numbers starting from zero\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    encoded_dataframe = data_preprocessing(task_1a_dataframe)\n",
    "    '''\n",
    "\n",
    "    # Create a copy of the input dataframe\n",
    "    encoded_dataframe = task_1a_dataframe.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Encode each categorical column\n",
    "    encoded_dataframe['Education'] = label_encoder.fit_transform(encoded_dataframe['Education'])\n",
    "    encoded_dataframe['City'] = label_encoder.fit_transform(encoded_dataframe['City'])\n",
    "    encoded_dataframe['Gender'] = label_encoder.fit_transform(encoded_dataframe['Gender'])\n",
    "    encoded_dataframe['EverBenched'] = label_encoder.fit_transform(encoded_dataframe['EverBenched'])\n",
    "    encoded_dataframe['PaymentTier'] = label_encoder.fit_transform(encoded_dataframe['PaymentTier'])\n",
    "    encoded_dataframe['ExperienceInCurrentDomain'] = label_encoder.fit_transform(encoded_dataframe['ExperienceInCurrentDomain'])\n",
    "    encoded_dataframe['LeaveOrNot'] = label_encoder.fit_transform(encoded_dataframe['LeaveOrNot'])\n",
    "\n",
    "    return encoded_dataframe\n",
    "\n",
    "def identify_features_and_targets(encoded_dataframe):\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    The purpose of this function is to define the features and\n",
    "    the required target labels. The function returns a python list\n",
    "    in which the first item is the selected features and second \n",
    "    item is the target label\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    `encoded_dataframe` : [ Dataframe ]\n",
    "                        Pandas dataframe that has all the features mapped to \n",
    "                        numbers starting from zero\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `features_and_targets` : [ list ]\n",
    "                            python list in which the first item is the \n",
    "                            selected features and second item is the target label\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    features_and_targets = identify_features_and_targets(encoded_dataframe)\n",
    "    '''\n",
    "\n",
    "    features = encoded_dataframe.drop(columns=['LeaveOrNot','Education','City','Age','Gender'])\n",
    "    target = encoded_dataframe['LeaveOrNot']\n",
    "    features_and_targets = [features, target]\n",
    "\n",
    "    return features_and_targets\n",
    "\n",
    "\n",
    "def load_as_tensors(features_and_targets):\n",
    "\n",
    "    ''' \n",
    "    Purpose:\n",
    "    ---\n",
    "    This function aims at loading your data (both training and validation)\n",
    "    as PyTorch tensors. Here you will have to split the dataset for training \n",
    "    and validation, and then load them as as tensors. \n",
    "    Training of the model requires iterating over the training tensors. \n",
    "    Hence the training sensors need to be converted to iterable dataset\n",
    "    object.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    `features_and targets` : [ list ]\n",
    "                            python list in which the first item is the \n",
    "                            selected features and second item is the target label\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `tensors_and_iterable_training_data` : [ list ]\n",
    "                                            Items:\n",
    "                                            [0]: X_train_tensor: Training features loaded into Pytorch array\n",
    "                                            [1]: X_test_tensor: Feature tensors in validation data\n",
    "                                            [2]: y_train_tensor: Training labels as Pytorch tensor\n",
    "                                            [3]: y_test_tensor: Target labels as tensor in validation data\n",
    "                                            [4]: Iterable dataset object and iterating over it in \n",
    "                                                batches, which are then fed into the model for processing\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    tensors_and_iterable_training_data = load_as_tensors(features_and_targets)\n",
    "    '''\n",
    "    features, target = features_and_targets\n",
    "    X_train1 = torch.tensor(features.values, dtype=torch.float32)\n",
    "    y_train1 = torch.tensor(target.values, dtype=torch.float32)\n",
    "    split_ratio = 0.8\n",
    "    split_index = int(len(X_train1) * split_ratio)\n",
    "    X_train_tensor, X_test_tensor = X_train1[:split_index], X_train1[split_index:]\n",
    "    y_train_tensor, y_test_tensor = y_train1[:split_index], y_train1[split_index:]\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    tensors_and_iterable_training_data = [X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, train_loader]\n",
    "\n",
    "    return tensors_and_iterable_training_data\n",
    "\n",
    "class Salary_Predictor(nn.Module):\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    The architecture and behavior of your neural network model will be\n",
    "    defined within this class that inherits from nn.Module. Here you\n",
    "    also need to specify how the input data is processed through the layers. \n",
    "    It defines the sequence of operations that transform the input data into \n",
    "    the predicted output. When an instance of this class is created and data\n",
    "    is passed through it, the `forward` method is automatically called, and \n",
    "    the output is the prediction of the model based on the input data.\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `predicted_output` : Predicted output for the given input data\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Salary_Predictor, self).__init__()\n",
    "        '''\n",
    "        Define the type and number of layers\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Define the activation functions\n",
    "        '''\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def model_loss_function():\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    To define the loss function for the model. Loss function measures \n",
    "    how well the predictions of a model match the actual target values \n",
    "    in training data.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `loss_function`: This can be a pre-defined loss function in PyTorch\n",
    "                    or can be user-defined\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    loss_function = model_loss_function()\n",
    "    '''\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "def model_optimizer(model):\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    To define the optimizer for the model. Optimizer is responsible \n",
    "    for updating the parameters (weights and biases) in a way that \n",
    "    minimizes the loss function.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    `model`: An object of the 'Salary_Predictor' class\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `optimizer`: Pre-defined optimizer from Pytorch\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    optimizer = model_optimizer(model)\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def model_number_of_epochs():\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    To define the number of epochs for training the model\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    `number_of_epochs`: [integer value]\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    number_of_epochs = model_number_of_epochs()\n",
    "        '''\n",
    "    number_of_epochs = 50\n",
    "\n",
    "    return number_of_epochs\n",
    "\n",
    "def training_function(model, number_of_epochs, tensors_and_iterable_training_data, loss_function, optimizer):\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    All the required parameters for training are passed to this function.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    1. `model`: An object of the 'Salary_Predictor' class\n",
    "    2. `number_of_epochs`: For training the model\n",
    "    3. `tensors_and_iterable_training_data`: list containing training and validation data tensors \n",
    "                                            and iterable dataset object of training tensors\n",
    "    4. `loss_function`: Loss function defined for the model\n",
    "    5. `optimizer`: Optimizer defined for the model\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    trained_model\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    trained_model = training_function(model, number_of_epochs, iterable_training_data, loss_function, optimizer)\n",
    "\n",
    "    '''\t\n",
    "    X_train_tensor, _, y_train_tensor, _ , train_loader = tensors_and_iterable_training_data\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        model.train() \n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_function(outputs, batch_y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{number_of_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def validation_function(trained_model, tensors_and_iterable_training_data):\n",
    "    '''\n",
    "    Purpose:\n",
    "    ---\n",
    "    This function will utilise the trained model to do predictions on the\n",
    "    validation dataset. This will enable us to understand the accuracy of\n",
    "    the model.\n",
    "\n",
    "    Input Arguments:\n",
    "    ---\n",
    "    1. `trained_model`: Returned from the training function\n",
    "    2. `tensors_and_iterable_training_data`: list containing training and validation data tensors \n",
    "                                            and iterable dataset object of training tensors\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    model_accuracy: Accuracy on the validation dataset\n",
    "\n",
    "    Example call:\n",
    "    ---\n",
    "    model_accuracy = validation_function(trained_model, tensors_and_iterable_training_data)\n",
    "\n",
    "    '''\t\n",
    "    _, X_test_tensor, _, y_test_tensor, _ = tensors_and_iterable_training_data\n",
    "\n",
    "    trained_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(X_test_tensor)\n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions = (predicted_labels == y_test_tensor.unsqueeze(1)).sum().item()\n",
    "        total_samples = len(y_test_tensor)\n",
    "        model_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Validation Accuracy: {model_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return model_accuracy\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "######### YOU ARE NOT ALLOWED TO MAKE CHANGES TO THIS FUNCTION #########\t\n",
    "'''\n",
    "    Purpose:\n",
    "    ---\n",
    "    The following is the main function combining all the functions\n",
    "    mentioned above. Go through this function to understand the flow\n",
    "    of the script\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # reading the provided dataset csv file using pandas library and \n",
    "    # converting it to a pandas Dataframe\n",
    "    task_1a_dataframe = pandas.read_csv('task_1a_dataset.csv')\n",
    "\n",
    "    # data preprocessing and obtaining encoded data\n",
    "    encoded_dataframe = data_preprocessing(task_1a_dataframe)\n",
    "\n",
    "    # selecting required features and targets\n",
    "    features_and_targets = identify_features_and_targets(encoded_dataframe)\n",
    "\n",
    "    # obtaining training and validation data tensors and the iterable\n",
    "    # training data object\n",
    "    tensors_and_iterable_training_data = load_as_tensors(features_and_targets)\n",
    "\n",
    "    # model is an instance of the class that defines the architecture of the model\n",
    "    model = Salary_Predictor()\n",
    "\n",
    "    # obtaining loss function, optimizer and the number of training epochs\n",
    "    loss_function = model_loss_function()\n",
    "    optimizer = model_optimizer(model)\n",
    "    number_of_epochs = model_number_of_epochs()\n",
    "\n",
    "    # training the model\n",
    "    trained_model = training_function(model, number_of_epochs, tensors_and_iterable_training_data, \n",
    "                    loss_function, optimizer)\n",
    "\n",
    "    # validating and obtaining accuracy\n",
    "    model_accuracy = validation_function(trained_model,tensors_and_iterable_training_data)\n",
    "    print(f\"Accuracy on the test set = {model_accuracy}\")\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_1306",
   "language": "python",
   "name": "gg_1306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
