{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160213bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 13s 1s/step - loss: 2.1897 - accuracy: 0.3469 - val_loss: 1.5473 - val_accuracy: 0.4750\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.4095 - accuracy: 0.4375 - val_loss: 0.8958 - val_accuracy: 0.6625\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.9540 - accuracy: 0.6281 - val_loss: 0.8697 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.8653 - accuracy: 0.6687 - val_loss: 0.6923 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7334 - accuracy: 0.7281 - val_loss: 0.6267 - val_accuracy: 0.7625\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.6664 - accuracy: 0.7594 - val_loss: 0.5866 - val_accuracy: 0.7875\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7152 - accuracy: 0.7156 - val_loss: 0.6642 - val_accuracy: 0.7625\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.6011 - accuracy: 0.7719 - val_loss: 0.5766 - val_accuracy: 0.7750\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.5628 - accuracy: 0.7906 - val_loss: 0.5528 - val_accuracy: 0.8250\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.5477 - accuracy: 0.8000 - val_loss: 0.6085 - val_accuracy: 0.8250\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 0s 37ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,  # Set to None to return images as they are\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)  # Extract the filename\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434ee518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\gupta\\\\Desktop\\\\Task 2A\\\\training\\\\Combat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder_path))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Count the number of images in each class\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_combat_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_images_in_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_combat_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m train_destroyedbuilding_count \u001b[38;5;241m=\u001b[39m count_images_in_folder(train_destroyedbuilding_dir)\n\u001b[0;32m     25\u001b[0m train_fire_count \u001b[38;5;241m=\u001b[39m count_images_in_folder(train_fire_dir)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mcount_images_in_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_images_in_folder\u001b[39m(folder_path):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\gupta\\\\Desktop\\\\Task 2A\\\\training\\\\Combat'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder paths for each class in the training dataset\n",
    "train_combat_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Combat\"\n",
    "train_destroyedbuilding_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\DestroyedBuildings\"\n",
    "train_fire_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Fire\"\n",
    "train_humanitarianaid_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Humanitarian Aid and rehabilitation\"\n",
    "train_militaryvehicles_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Military vehicles and weapons\"\n",
    "\n",
    "# Define the folder paths for each class in the testing dataset\n",
    "test_combat_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\combat\"\n",
    "test_destroyedbuilding_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\destroyedbuilding\"\n",
    "test_fire_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\fire\"\n",
    "test_humanitarianaid_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\humanitarianaid\"\n",
    "test_militaryvehicles_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\militaryvehicles\" \n",
    "\n",
    "\n",
    "# Function to count the number of images in a folder\n",
    "def count_images_in_folder(folder_path):\n",
    "    return len(os.listdir(folder_path))\n",
    "\n",
    "# Count the number of images in each class\n",
    "train_combat_count = count_images_in_folder(train_combat_dir)\n",
    "train_destroyedbuilding_count = count_images_in_folder(train_destroyedbuilding_dir)\n",
    "train_fire_count = count_images_in_folder(train_fire_dir)\n",
    "train_humanitarianaid_count = count_images_in_folder(train_humanitarianaid_dir)\n",
    "train_militaryvehicles_count = count_images_in_folder(train_militaryvehicles_dir)\n",
    "\n",
    "test_combat_count = count_images_in_folder(test_combat_dir)\n",
    "test_destroyedbuilding_count = count_images_in_folder(test_destroyedbuilding_dir)\n",
    "test_fire_count = count_images_in_folder(test_fire_dir)\n",
    "test_humanitarianaid_count = count_images_in_folder(test_humanitarianaid_dir)\n",
    "test_militaryvehicles_count = count_images_in_folder(test_militaryvehicles_dir)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Training Data:\")\n",
    "print(f\"Combat: {train_combat_count} images\")\n",
    "print(f\"Destroyed Building: {train_destroyedbuilding_count} images\")\n",
    "print(f\"Fire: {train_fire_count} images\")\n",
    "print(f\"Humanitarian Aid: {train_humanitarianaid_count} images\")\n",
    "print(f\"Military Vehicles: {train_militaryvehicles_count} images\")\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "print(f\"Combat: {test_combat_count} images\")\n",
    "print(f\"Destroyed Building: {test_destroyedbuilding_count} images\")\n",
    "print(f\"Fire: {test_fire_count} images\")\n",
    "print(f\"Humanitarian Aid: {test_humanitarianaid_count} images\")\n",
    "print(f\"Military Vehicles: {test_militaryvehicles_count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb253e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96082f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d0678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 15s 1s/step - loss: 2.2004 - accuracy: 0.3250 - val_loss: 1.1160 - val_accuracy: 0.5875\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 15s 1s/step - loss: 1.2701 - accuracy: 0.5000 - val_loss: 0.8618 - val_accuracy: 0.7000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.9892 - accuracy: 0.6313 - val_loss: 0.7564 - val_accuracy: 0.7750\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.8883 - accuracy: 0.6719 - val_loss: 0.6721 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7230 - accuracy: 0.7250 - val_loss: 0.7085 - val_accuracy: 0.7625\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7108 - accuracy: 0.7125 - val_loss: 0.4985 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6530 - accuracy: 0.7656 - val_loss: 0.5242 - val_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6671 - accuracy: 0.7531 - val_loss: 0.5345 - val_accuracy: 0.8250\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.5843 - accuracy: 0.7875 - val_loss: 0.5100 - val_accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.4737 - accuracy: 0.8375 - val_loss: 0.6230 - val_accuracy: 0.7625\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 1s 43ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: combat\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,  # Set to None to return images as they are\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)  # Extract the filename\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc213cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7400a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58063c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c5fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 46s 4s/step - loss: 1.5518 - accuracy: 0.4563 - val_loss: 0.6724 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.8503 - accuracy: 0.6844 - val_loss: 0.2446 - val_accuracy: 0.9125 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5029 - accuracy: 0.8156 - val_loss: 0.1433 - val_accuracy: 0.9625 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3677 - accuracy: 0.8844 - val_loss: 0.1714 - val_accuracy: 0.9500 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.4201 - accuracy: 0.8594 - val_loss: 0.1211 - val_accuracy: 0.9500 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2474 - accuracy: 0.9187 - val_loss: 0.2061 - val_accuracy: 0.9250 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2458 - accuracy: 0.9156 - val_loss: 0.2318 - val_accuracy: 0.9375 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1879 - accuracy: 0.9312 - val_loss: 0.1547 - val_accuracy: 0.9500 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.1742 - val_accuracy: 0.9250 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1570 - accuracy: 0.9563 - val_loss: 0.1179 - val_accuracy: 0.9625 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1436 - accuracy: 0.9563 - val_loss: 0.2447 - val_accuracy: 0.9250 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.1202 - accuracy: 0.9594 - val_loss: 0.0961 - val_accuracy: 0.9625 - lr: 3.3287e-04\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1091 - accuracy: 0.9688 - val_loss: 0.1962 - val_accuracy: 0.9500 - lr: 3.0119e-04\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.1128 - accuracy: 0.9719 - val_loss: 0.1594 - val_accuracy: 0.9625 - lr: 2.7253e-04\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1239 - accuracy: 0.9750 - val_loss: 0.2363 - val_accuracy: 0.9250 - lr: 2.4660e-04\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0910 - accuracy: 0.9781 - val_loss: 0.1596 - val_accuracy: 0.9500 - lr: 2.2313e-04\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0787 - accuracy: 0.9781 - val_loss: 0.1488 - val_accuracy: 0.9375 - lr: 2.0190e-04\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 6s 157ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "#Model most accurate\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,  # Increased rotation range\n",
    "    width_shift_range=0.3,  # Increased width shift range\n",
    "    height_shift_range=0.3,  # Increased height shift range\n",
    "    shear_range=0.3,  # Increased shear range\n",
    "    zoom_range=0.3,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Changed to Global Average Pooling\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:  # Fine-tuning last few layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee536b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 102>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m    100\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    112\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malien_attack_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1851\u001b[0m, in \u001b[0;36mImageDataGenerator.standardize\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_function(x)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale:\n\u001b[1;32m-> 1851\u001b[0m     x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamplewise_center:\n\u001b[0;32m   1853\u001b[0m     x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(x, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import cv2\n",
    "\n",
    "# Define custom preprocessing functions\n",
    "def adjust_color_balance(image, alpha=1.2, beta=10):\n",
    "    result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def gamma_correction(image, gamma=1.2):\n",
    "    table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Check if the image is in uint8 format\n",
    "    if image.dtype == np.uint8:\n",
    "        # Apply your custom preprocessing steps\n",
    "        image = adjust_color_balance(image, alpha=1.2, beta=10)\n",
    "        image = gamma_correction(image, gamma=1.2)\n",
    "        image = cv2.medianBlur(image, 1)  # Uncomment if you want to apply median blur\n",
    "        return image\n",
    "    else:\n",
    "        # Convert float32 image to uint8 before preprocessing\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        # Apply your custom preprocessing steps\n",
    "        image = adjust_color_balance(image, alpha=1.2, beta=10)\n",
    "        image = gamma_correction(image, gamma=1.2)\n",
    "        image = cv2.medianBlur(image, 1)  # Uncomment if you want to apply median blur\n",
    "        return image\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    "    preprocessing_function=preprocess_image  # Apply custom preprocessing\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255, preprocessing_function=preprocess_image)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58037d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 129ms/step\n",
      "Predicted Probabilities:\n",
      "[[9.92911041e-01 5.26108139e-04 3.82176077e-05 6.46373769e-03\n",
      "  6.08649716e-05]\n",
      " [9.78283644e-01 1.47285391e-04 6.00530242e-04 2.08244696e-02\n",
      "  1.44126228e-04]\n",
      " [2.69469838e-05 9.99913812e-01 4.86385534e-06 7.03069372e-06\n",
      "  4.72923894e-05]\n",
      " [6.01065551e-07 9.99943018e-01 8.28067527e-07 7.37412904e-07\n",
      "  5.48284152e-05]\n",
      " [1.20492163e-03 8.22759321e-05 9.98250425e-01 1.31958688e-04\n",
      "  3.30484472e-04]\n",
      " [6.39594000e-05 1.92162770e-05 9.99900579e-01 3.73626744e-06\n",
      "  1.24461039e-05]\n",
      " [3.90748377e-04 7.20346463e-07 7.15447895e-05 9.99515772e-01\n",
      "  2.12488958e-05]\n",
      " [1.08971319e-03 3.00853018e-04 2.22294460e-04 9.94901299e-01\n",
      "  3.48585332e-03]\n",
      " [7.34713860e-04 7.65984505e-03 8.55154125e-04 1.15404364e-04\n",
      "  9.90634859e-01]\n",
      " [4.04481834e-05 1.79461800e-04 7.27231090e-05 7.42806078e-05\n",
      "  9.99633074e-01]]\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Print predicted probabilities for each class\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(test_predictions)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d77d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b5d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 51s 4s/step - loss: 1.4734 - accuracy: 0.4369 - val_loss: 0.4973 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.7671 - accuracy: 0.7138 - val_loss: 0.3367 - val_accuracy: 0.9125 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5056 - accuracy: 0.8185 - val_loss: 0.2746 - val_accuracy: 0.9125 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4724 - accuracy: 0.8462 - val_loss: 0.2298 - val_accuracy: 0.9250 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.3656 - accuracy: 0.8738 - val_loss: 0.1009 - val_accuracy: 0.9625 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.2959 - accuracy: 0.9108 - val_loss: 0.1898 - val_accuracy: 0.9375 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.2712 - accuracy: 0.8985 - val_loss: 0.1846 - val_accuracy: 0.9125 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.2293 - accuracy: 0.9262 - val_loss: 0.2207 - val_accuracy: 0.9250 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.2406 - accuracy: 0.9169 - val_loss: 0.1584 - val_accuracy: 0.9500 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.1846 - accuracy: 0.9385 - val_loss: 0.1283 - val_accuracy: 0.9500 - lr: 4.0657e-04\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 6s 318ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,  # Increased rotation range\n",
    "    width_shift_range=0.3,  # Increased width shift range\n",
    "    height_shift_range=0.3,  # Increased height shift range\n",
    "    shear_range=0.3,  # Increased shear range\n",
    "    zoom_range=0.3,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Changed to Global Average Pooling\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:  # Fine-tuning last few layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095fd9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 65s 5s/step - loss: 1.4902 - accuracy: 0.4187 - val_loss: 0.7569 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.7599 - accuracy: 0.7406 - val_loss: 0.4078 - val_accuracy: 0.8500 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.6702 - accuracy: 0.7437 - val_loss: 0.3074 - val_accuracy: 0.9000 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.5440 - accuracy: 0.8125 - val_loss: 0.3719 - val_accuracy: 0.8875 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.3964 - accuracy: 0.8656 - val_loss: 0.3229 - val_accuracy: 0.8625 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.3775 - accuracy: 0.8594 - val_loss: 0.2539 - val_accuracy: 0.9250 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.3117 - accuracy: 0.9062 - val_loss: 0.1676 - val_accuracy: 0.9500 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2601 - accuracy: 0.9250 - val_loss: 0.2928 - val_accuracy: 0.9000 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2610 - accuracy: 0.9187 - val_loss: 0.1882 - val_accuracy: 0.9250 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2520 - accuracy: 0.9000 - val_loss: 0.2606 - val_accuracy: 0.8875 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2360 - accuracy: 0.9094 - val_loss: 0.2630 - val_accuracy: 0.9000 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2185 - accuracy: 0.9156 - val_loss: 0.1581 - val_accuracy: 0.9375 - lr: 3.3287e-04\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.2383 - accuracy: 0.9375 - val_loss: 0.1878 - val_accuracy: 0.9250 - lr: 3.0119e-04\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.1774 - accuracy: 0.9375 - val_loss: 0.2024 - val_accuracy: 0.8875 - lr: 2.7253e-04\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1330 - accuracy: 0.9594 - val_loss: 0.1521 - val_accuracy: 0.9125 - lr: 2.4660e-04\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1652 - accuracy: 0.9469 - val_loss: 0.2329 - val_accuracy: 0.9250 - lr: 2.2313e-04\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1779 - accuracy: 0.9375 - val_loss: 0.1433 - val_accuracy: 0.9625 - lr: 2.0190e-04\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1918 - accuracy: 0.9312 - val_loss: 0.1791 - val_accuracy: 0.9250 - lr: 1.8268e-04\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1991 - accuracy: 0.9281 - val_loss: 0.2274 - val_accuracy: 0.9125 - lr: 1.6530e-04\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1072 - accuracy: 0.9719 - val_loss: 0.2315 - val_accuracy: 0.9125 - lr: 1.4957e-04\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<0,2,5>,3>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 110>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m    109\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    111\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Image\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Preprocess the image and make predictions\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<0,2,5>,3>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (200, 200)  # Adjusted resolution\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255  # Rescale the pixel values to the range [0, 1]\n",
    ")\n",
    "\n",
    "# Only rescale for testing data\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(200, 200, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare a sample testing image for visualization\n",
    "test_img_path = r\"C:\\Users\\gupta\\Desktop\\Eyantra_GG(2023)\\Task_2B\\testing\\fire\\fire1.jpeg\"\n",
    "test_img = cv2.imread(test_img_path)\n",
    "test_img = cv2.resize(test_img, (200, 200))  # Resize the image to the model's input size\n",
    "test_img = test_img / 255.0  # Normalize pixel values\n",
    "\n",
    "# Visualize the original image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Preprocess the image and make predictions\n",
    "processed_img = test_img[np.newaxis, ...]\n",
    "predicted_probabilities = model.predict(processed_img)\n",
    "predicted_class = np.argmax(predicted_probabilities)\n",
    "predicted_label = class_names[predicted_class]\n",
    "\n",
    "# Visualize the processed image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Processed Image\\nPredicted Class: {predicted_label}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0fc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a47e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.2178 - accuracy: 0.5750 - val_loss: 1.2915 - val_accuracy: 0.6625 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 12s 1s/step - loss: 0.4851 - accuracy: 0.8469 - val_loss: 0.6756 - val_accuracy: 0.7750 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.4234 - accuracy: 0.8562 - val_loss: 0.4668 - val_accuracy: 0.8625 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2616 - accuracy: 0.8906 - val_loss: 1.0742 - val_accuracy: 0.7750 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2826 - accuracy: 0.9125 - val_loss: 0.9105 - val_accuracy: 0.7750 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2480 - accuracy: 0.9187 - val_loss: 1.0142 - val_accuracy: 0.8000 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2098 - accuracy: 0.9375 - val_loss: 0.4345 - val_accuracy: 0.8625 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1900 - accuracy: 0.9469 - val_loss: 1.0685 - val_accuracy: 0.8125 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1269 - accuracy: 0.9469 - val_loss: 1.1639 - val_accuracy: 0.8125 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.1837 - accuracy: 0.9281 - val_loss: 1.3239 - val_accuracy: 0.7875 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1692 - accuracy: 0.9406 - val_loss: 0.7588 - val_accuracy: 0.9000 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1216 - accuracy: 0.9594 - val_loss: 1.3025 - val_accuracy: 0.8500 - lr: 3.3287e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 1s 46ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (MobileNetV2)\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Visualize training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af28f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd23623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 5 classes.\n",
      "400/400 [==============================] - 38s 90ms/step\n",
      "Classification Report (Training Data):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Combat       1.00      1.00      1.00        80\n",
      "             Fire       1.00      1.00      1.00        80\n",
      "destroyedbuilding       1.00      1.00      1.00        80\n",
      "  humanitarianaid       1.00      1.00      1.00        80\n",
      " militaryvehicles       1.00      1.00      1.00        80\n",
      "\n",
      "         accuracy                           1.00       400\n",
      "        macro avg       1.00      1.00      1.00       400\n",
      "     weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "Confusion Matrix (Training Data):\n",
      "[[80  0  0  0  0]\n",
      " [ 0 80  0  0  0]\n",
      " [ 0  0 80  0  0]\n",
      " [ 0  0  0 80  0]\n",
      " [ 0  0  0  0 80]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 1\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the training data\n",
    "train_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    \"training\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on training data\n",
    "train_predictions = model.predict(train_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [os.path.basename(os.path.dirname(path)) for path in train_generator.filepaths]\n",
    "\n",
    "# Get true labels\n",
    "true_labels = [os.path.basename(os.path.dirname(path)) for path in train_generator.filepaths]\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report (Training Data):\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "print(\"Confusion Matrix (Training Data):\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
