{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160213bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 13s 1s/step - loss: 2.1897 - accuracy: 0.3469 - val_loss: 1.5473 - val_accuracy: 0.4750\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.4095 - accuracy: 0.4375 - val_loss: 0.8958 - val_accuracy: 0.6625\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.9540 - accuracy: 0.6281 - val_loss: 0.8697 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.8653 - accuracy: 0.6687 - val_loss: 0.6923 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7334 - accuracy: 0.7281 - val_loss: 0.6267 - val_accuracy: 0.7625\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.6664 - accuracy: 0.7594 - val_loss: 0.5866 - val_accuracy: 0.7875\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.7152 - accuracy: 0.7156 - val_loss: 0.6642 - val_accuracy: 0.7625\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.6011 - accuracy: 0.7719 - val_loss: 0.5766 - val_accuracy: 0.7750\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.5628 - accuracy: 0.7906 - val_loss: 0.5528 - val_accuracy: 0.8250\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.5477 - accuracy: 0.8000 - val_loss: 0.6085 - val_accuracy: 0.8250\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 0s 37ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,  # Set to None to return images as they are\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)  # Extract the filename\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434ee518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\gupta\\\\Desktop\\\\Task 2A\\\\training\\\\Combat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder_path))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Count the number of images in each class\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_combat_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_images_in_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_combat_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m train_destroyedbuilding_count \u001b[38;5;241m=\u001b[39m count_images_in_folder(train_destroyedbuilding_dir)\n\u001b[0;32m     25\u001b[0m train_fire_count \u001b[38;5;241m=\u001b[39m count_images_in_folder(train_fire_dir)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mcount_images_in_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_images_in_folder\u001b[39m(folder_path):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\gupta\\\\Desktop\\\\Task 2A\\\\training\\\\Combat'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder paths for each class in the training dataset\n",
    "train_combat_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Combat\"\n",
    "train_destroyedbuilding_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\DestroyedBuildings\"\n",
    "train_fire_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Fire\"\n",
    "train_humanitarianaid_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Humanitarian Aid and rehabilitation\"\n",
    "train_militaryvehicles_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\training\\Military vehicles and weapons\"\n",
    "\n",
    "# Define the folder paths for each class in the testing dataset\n",
    "test_combat_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\combat\"\n",
    "test_destroyedbuilding_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\destroyedbuilding\"\n",
    "test_fire_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\fire\"\n",
    "test_humanitarianaid_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\humanitarianaid\"\n",
    "test_militaryvehicles_dir = r\"C:\\Users\\gupta\\Desktop\\Task 2A\\testing\\militaryvehicles\" \n",
    "\n",
    "\n",
    "# Function to count the number of images in a folder\n",
    "def count_images_in_folder(folder_path):\n",
    "    return len(os.listdir(folder_path))\n",
    "\n",
    "# Count the number of images in each class\n",
    "train_combat_count = count_images_in_folder(train_combat_dir)\n",
    "train_destroyedbuilding_count = count_images_in_folder(train_destroyedbuilding_dir)\n",
    "train_fire_count = count_images_in_folder(train_fire_dir)\n",
    "train_humanitarianaid_count = count_images_in_folder(train_humanitarianaid_dir)\n",
    "train_militaryvehicles_count = count_images_in_folder(train_militaryvehicles_dir)\n",
    "\n",
    "test_combat_count = count_images_in_folder(test_combat_dir)\n",
    "test_destroyedbuilding_count = count_images_in_folder(test_destroyedbuilding_dir)\n",
    "test_fire_count = count_images_in_folder(test_fire_dir)\n",
    "test_humanitarianaid_count = count_images_in_folder(test_humanitarianaid_dir)\n",
    "test_militaryvehicles_count = count_images_in_folder(test_militaryvehicles_dir)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Training Data:\")\n",
    "print(f\"Combat: {train_combat_count} images\")\n",
    "print(f\"Destroyed Building: {train_destroyedbuilding_count} images\")\n",
    "print(f\"Fire: {train_fire_count} images\")\n",
    "print(f\"Humanitarian Aid: {train_humanitarianaid_count} images\")\n",
    "print(f\"Military Vehicles: {train_militaryvehicles_count} images\")\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "print(f\"Combat: {test_combat_count} images\")\n",
    "print(f\"Destroyed Building: {test_destroyedbuilding_count} images\")\n",
    "print(f\"Fire: {test_fire_count} images\")\n",
    "print(f\"Humanitarian Aid: {test_humanitarianaid_count} images\")\n",
    "print(f\"Military Vehicles: {test_militaryvehicles_count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb253e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96082f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d0678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 15s 1s/step - loss: 2.2004 - accuracy: 0.3250 - val_loss: 1.1160 - val_accuracy: 0.5875\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 15s 1s/step - loss: 1.2701 - accuracy: 0.5000 - val_loss: 0.8618 - val_accuracy: 0.7000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.9892 - accuracy: 0.6313 - val_loss: 0.7564 - val_accuracy: 0.7750\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.8883 - accuracy: 0.6719 - val_loss: 0.6721 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7230 - accuracy: 0.7250 - val_loss: 0.7085 - val_accuracy: 0.7625\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7108 - accuracy: 0.7125 - val_loss: 0.4985 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6530 - accuracy: 0.7656 - val_loss: 0.5242 - val_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6671 - accuracy: 0.7531 - val_loss: 0.5345 - val_accuracy: 0.8250\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.5843 - accuracy: 0.7875 - val_loss: 0.5100 - val_accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.4737 - accuracy: 0.8375 - val_loss: 0.6230 - val_accuracy: 0.7625\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 1s 43ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: combat\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,  # Set to None to return images as they are\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)  # Extract the filename\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc213cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7400a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58063c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c5fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 46s 4s/step - loss: 1.5518 - accuracy: 0.4563 - val_loss: 0.6724 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.8503 - accuracy: 0.6844 - val_loss: 0.2446 - val_accuracy: 0.9125 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5029 - accuracy: 0.8156 - val_loss: 0.1433 - val_accuracy: 0.9625 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3677 - accuracy: 0.8844 - val_loss: 0.1714 - val_accuracy: 0.9500 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.4201 - accuracy: 0.8594 - val_loss: 0.1211 - val_accuracy: 0.9500 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2474 - accuracy: 0.9187 - val_loss: 0.2061 - val_accuracy: 0.9250 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2458 - accuracy: 0.9156 - val_loss: 0.2318 - val_accuracy: 0.9375 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1879 - accuracy: 0.9312 - val_loss: 0.1547 - val_accuracy: 0.9500 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.1742 - val_accuracy: 0.9250 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1570 - accuracy: 0.9563 - val_loss: 0.1179 - val_accuracy: 0.9625 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1436 - accuracy: 0.9563 - val_loss: 0.2447 - val_accuracy: 0.9250 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.1202 - accuracy: 0.9594 - val_loss: 0.0961 - val_accuracy: 0.9625 - lr: 3.3287e-04\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1091 - accuracy: 0.9688 - val_loss: 0.1962 - val_accuracy: 0.9500 - lr: 3.0119e-04\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.1128 - accuracy: 0.9719 - val_loss: 0.1594 - val_accuracy: 0.9625 - lr: 2.7253e-04\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1239 - accuracy: 0.9750 - val_loss: 0.2363 - val_accuracy: 0.9250 - lr: 2.4660e-04\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0910 - accuracy: 0.9781 - val_loss: 0.1596 - val_accuracy: 0.9500 - lr: 2.2313e-04\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0787 - accuracy: 0.9781 - val_loss: 0.1488 - val_accuracy: 0.9375 - lr: 2.0190e-04\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 6s 157ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "#ModelNormal\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,  # Increased rotation range\n",
    "    width_shift_range=0.3,  # Increased width shift range\n",
    "    height_shift_range=0.3,  # Increased height shift range\n",
    "    shear_range=0.3,  # Increased shear range\n",
    "    zoom_range=0.3,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Changed to Global Average Pooling\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:  # Fine-tuning last few layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684626fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 221s 14s/step - loss: 1.3781 - accuracy: 0.4923 - val_loss: 283.7086 - val_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 132s 12s/step - loss: 1.0685 - accuracy: 0.6431 - val_loss: 977.1644 - val_accuracy: 0.2125 - lr: 9.0484e-04\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 136s 12s/step - loss: 1.1796 - accuracy: 0.5692 - val_loss: 49.5777 - val_accuracy: 0.1375 - lr: 8.1873e-04\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 145s 13s/step - loss: 0.9203 - accuracy: 0.6646 - val_loss: 57.1243 - val_accuracy: 0.2625 - lr: 7.4082e-04\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 157s 14s/step - loss: 0.9711 - accuracy: 0.7169 - val_loss: 62.9740 - val_accuracy: 0.2375 - lr: 6.7032e-04\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 162s 15s/step - loss: 0.9410 - accuracy: 0.6831 - val_loss: 24.9476 - val_accuracy: 0.2250 - lr: 6.0653e-04\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 162s 15s/step - loss: 0.7373 - accuracy: 0.7231 - val_loss: 16.9164 - val_accuracy: 0.4125 - lr: 5.4881e-04\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 141s 12s/step - loss: 0.6879 - accuracy: 0.7877 - val_loss: 9.8160 - val_accuracy: 0.4750 - lr: 4.9659e-04\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 128s 12s/step - loss: 0.8414 - accuracy: 0.7385 - val_loss: 21.6956 - val_accuracy: 0.2750 - lr: 4.4933e-04\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 127s 12s/step - loss: 0.7130 - accuracy: 0.7754 - val_loss: 18.1158 - val_accuracy: 0.2375 - lr: 4.0657e-04\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 131s 12s/step - loss: 0.4914 - accuracy: 0.8462 - val_loss: 4.5395 - val_accuracy: 0.4500 - lr: 3.6788e-04\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 131s 13s/step - loss: 0.3982 - accuracy: 0.8708 - val_loss: 4.6178 - val_accuracy: 0.5125 - lr: 3.3287e-04\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 146s 13s/step - loss: 0.4051 - accuracy: 0.8554 - val_loss: 2.4212 - val_accuracy: 0.6250 - lr: 3.0119e-04\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 154s 14s/step - loss: 0.3820 - accuracy: 0.8677 - val_loss: 2.9543 - val_accuracy: 0.4875 - lr: 2.7253e-04\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 155s 14s/step - loss: 0.3645 - accuracy: 0.8923 - val_loss: 2.0019 - val_accuracy: 0.6375 - lr: 2.4660e-04\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 169s 17s/step - loss: 0.4305 - accuracy: 0.8492 - val_loss: 1.8726 - val_accuracy: 0.6750 - lr: 2.2313e-04\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 143s 13s/step - loss: 0.2997 - accuracy: 0.9046 - val_loss: 1.3173 - val_accuracy: 0.7000 - lr: 2.0190e-04\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 126s 11s/step - loss: 0.2388 - accuracy: 0.9169 - val_loss: 0.8592 - val_accuracy: 0.8250 - lr: 1.8268e-04\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 126s 11s/step - loss: 0.2550 - accuracy: 0.9354 - val_loss: 0.4024 - val_accuracy: 0.9125 - lr: 1.6530e-04\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 127s 11s/step - loss: 0.3393 - accuracy: 0.8831 - val_loss: 1.1849 - val_accuracy: 0.7750 - lr: 1.4957e-04\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 143s 13s/step - loss: 0.3362 - accuracy: 0.8954 - val_loss: 2.3964 - val_accuracy: 0.6625 - lr: 1.3534e-04\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 131s 13s/step - loss: 0.2835 - accuracy: 0.9169 - val_loss: 1.3186 - val_accuracy: 0.7125 - lr: 1.2246e-04\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 132s 12s/step - loss: 0.3387 - accuracy: 0.9077 - val_loss: 0.9537 - val_accuracy: 0.7125 - lr: 1.1080e-04\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 138s 13s/step - loss: 0.2504 - accuracy: 0.9200 - val_loss: 0.6193 - val_accuracy: 0.8125 - lr: 1.0026e-04\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 160s 14s/step - loss: 0.2285 - accuracy: 0.9262 - val_loss: 0.6478 - val_accuracy: 0.8125 - lr: 9.0718e-05\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 166s 15s/step - loss: 0.1710 - accuracy: 0.9508 - val_loss: 0.7428 - val_accuracy: 0.8125 - lr: 8.2085e-05\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 131s 12s/step - loss: 0.2018 - accuracy: 0.9415 - val_loss: 0.6548 - val_accuracy: 0.8125 - lr: 7.4274e-05\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 132s 12s/step - loss: 0.1971 - accuracy: 0.9323 - val_loss: 0.7582 - val_accuracy: 0.7875 - lr: 6.7206e-05\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 130s 12s/step - loss: 0.1886 - accuracy: 0.9354 - val_loss: 0.5975 - val_accuracy: 0.8375 - lr: 6.0810e-05\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 7s 158ms/step\n",
      "Image: combat1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: combat2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "#image Preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "import cv2\n",
    "\n",
    "# Define custom preprocessing functions\n",
    "def adjust_color_balance(image, alpha=1.2, beta=10):\n",
    "    result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def gamma_correction(image, gamma=1.2):\n",
    "    table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Apply your custom preprocessing steps\n",
    "    image = adjust_color_balance(image, alpha=1.2, beta=10)\n",
    "    image = gamma_correction(image, gamma=1.2)\n",
    "    image = cv2.medianBlur(image, 3)  # Uncomment if you want to apply median blur\n",
    "    return image\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=None,\n",
    "    rotation_range=50,  # Increased rotation range\n",
    "    width_shift_range=0.4,  # Increased width shift range\n",
    "    height_shift_range=0.4,  # Increased height shift range\n",
    "    shear_range=0.4,  # Increased shear range\n",
    "    zoom_range=0.4,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    "    preprocessing_function=preprocess_image  # Apply custom preprocessing\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers[-(len(base_model.layers) - 10):]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),  # Increase the number of units\n",
    "    Dropout(0.5),  # Add dropout for regularization\n",
    "    Dense(256, activation='relu'),  # Add another dense layer\n",
    "    Dropout(0.3),  # Add dropout for regularization\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=None, preprocessing_function=preprocess_image)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21544333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 405 images belonging to 5 classes.\n",
      "Found 0 images belonging to 5 classes.\n",
      "Class Weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Epoch 1/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6152 - accuracy: 0.3531 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 203s 12s/step - loss: 1.6152 - accuracy: 0.3531 - lr: 1.0000e-04\n",
      "Epoch 2/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.7457 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 160s 13s/step - loss: 0.7933 - accuracy: 0.7457 - lr: 9.0484e-05\n",
      "Epoch 3/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.8765 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 153s 12s/step - loss: 0.4422 - accuracy: 0.8765 - lr: 8.1873e-05\n",
      "Epoch 4/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9556 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 158s 12s/step - loss: 0.2379 - accuracy: 0.9556 - lr: 7.4082e-05\n",
      "Epoch 5/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9358 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 158s 12s/step - loss: 0.2098 - accuracy: 0.9358 - lr: 6.7032e-05\n",
      "Epoch 6/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9728 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 157s 12s/step - loss: 0.1119 - accuracy: 0.9728 - lr: 6.0653e-05\n",
      "Epoch 7/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9901 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 157s 12s/step - loss: 0.0813 - accuracy: 0.9901 - lr: 5.4881e-05\n",
      "Epoch 8/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9951 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 158s 12s/step - loss: 0.0739 - accuracy: 0.9951 - lr: 4.9659e-05\n",
      "Epoch 9/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9951 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 174s 13s/step - loss: 0.0480 - accuracy: 0.9951 - lr: 4.4933e-05\n",
      "Epoch 10/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9926 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 184s 14s/step - loss: 0.0480 - accuracy: 0.9926 - lr: 4.0657e-05\n",
      "Epoch 11/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9951 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 185s 14s/step - loss: 0.0493 - accuracy: 0.9951 - lr: 3.6788e-05\n",
      "Epoch 12/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9951 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 187s 14s/step - loss: 0.0386 - accuracy: 0.9951 - lr: 3.3287e-05\n",
      "Epoch 13/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 186s 14s/step - loss: 0.0277 - accuracy: 1.0000 - lr: 3.0119e-05\n",
      "Epoch 14/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9901 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 186s 14s/step - loss: 0.0493 - accuracy: 0.9901 - lr: 2.7253e-05\n",
      "Epoch 15/15\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9926 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "13/13 [==============================] - 189s 14s/step - loss: 0.0351 - accuracy: 0.9926 - lr: 2.4660e-05\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 12s 550ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define custom preprocessing functions\n",
    "def adjust_color_balance(image, alpha=1.2, beta=10):\n",
    "    result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def gamma_correction(image, gamma=1.2):\n",
    "    table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "Ndef preprocess_image(image):\n",
    "    # Apply your custom preprocessing steps\n",
    "    image = adjust_color_balance(image, alpha=1.2, beta=10)\n",
    "    image = gamma_correction(image, gamma=1.2)\n",
    "    image = cv2.medianBlur(image, 3)  # Uncomment if you want to apply median blur\n",
    "    return image\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rescale=None,preprocessing_function=preprocess_image)\n",
    "test_data_gen = ImageDataGenerator(rescale=None,preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Fine-tune more layers\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Use class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_names))}\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Adjust learning rate\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.0001 * np.exp(-epoch / 10)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Use class weights during training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping],\n",
    "    class_weight=class_weights_dict \n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051a4c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038a66d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           combat       0.00      0.00      0.00         2\n",
      "destroyedbuilding       0.00      0.00      0.00         2\n",
      "             fire       0.00      0.00      0.00         2\n",
      "  humanitarianaid       0.00      0.00      0.00         2\n",
      " militaryvehicles       0.22      1.00      0.36         2\n",
      "\n",
      "         accuracy                           0.20        10\n",
      "        macro avg       0.04      0.20      0.07        10\n",
      "     weighted avg       0.04      0.20      0.07        10\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 0 0 0 2]\n",
      " [0 0 0 0 2]\n",
      " [0 0 0 0 2]\n",
      " [0 0 1 0 1]\n",
      " [0 0 0 0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Get true labels\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels_numeric = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels_numeric)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels_numeric, target_names=class_names))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef988521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "def adjust_color_balance(image, alpha=1.2, beta=10):\n",
    "    result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def gamma_correction(image, gamma=1.2):\n",
    "    table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Apply your custom preprocessing steps\n",
    "    image = adjust_color_balance(image, alpha=1.2, beta=10)\n",
    "    image = gamma_correction(image, gamma=1.2)\n",
    "    image = cv2.medianBlur(image, 3)  # Uncomment if you want to apply median blur\n",
    "    return image\n",
    "\n",
    "# Load an example image\n",
    "img_path = r\"C:\\Users\\gupta\\Desktop\\Eyantra_GG(2023)\\geoguide_1306\\Task_2B\\training\\Fire\\81.jpg\"\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Apply custom preprocessing\n",
    "# img_array = adjust_color_balance(img_array, alpha=1.2, beta=10)\n",
    "# img_array = gamma_correction(img_array, gamma=1.2)\n",
    "# img_array = cv2.medianBlur(img_array, 3)  # Uncomment if you want to apply median blur\n",
    "\n",
    "# Add an extra dimension and preprocess the input for VGG16\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "# img_array = preprocess_input(img_array)\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(img_array[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58037d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 129ms/step\n",
      "Predicted Probabilities:\n",
      "[[9.92911041e-01 5.26108139e-04 3.82176077e-05 6.46373769e-03\n",
      "  6.08649716e-05]\n",
      " [9.78283644e-01 1.47285391e-04 6.00530242e-04 2.08244696e-02\n",
      "  1.44126228e-04]\n",
      " [2.69469838e-05 9.99913812e-01 4.86385534e-06 7.03069372e-06\n",
      "  4.72923894e-05]\n",
      " [6.01065551e-07 9.99943018e-01 8.28067527e-07 7.37412904e-07\n",
      "  5.48284152e-05]\n",
      " [1.20492163e-03 8.22759321e-05 9.98250425e-01 1.31958688e-04\n",
      "  3.30484472e-04]\n",
      " [6.39594000e-05 1.92162770e-05 9.99900579e-01 3.73626744e-06\n",
      "  1.24461039e-05]\n",
      " [3.90748377e-04 7.20346463e-07 7.15447895e-05 9.99515772e-01\n",
      "  2.12488958e-05]\n",
      " [1.08971319e-03 3.00853018e-04 2.22294460e-04 9.94901299e-01\n",
      "  3.48585332e-03]\n",
      " [7.34713860e-04 7.65984505e-03 8.55154125e-04 1.15404364e-04\n",
      "  9.90634859e-01]\n",
      " [4.04481834e-05 1.79461800e-04 7.27231090e-05 7.42806078e-05\n",
      "  9.99633074e-01]]\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Print predicted probabilities for each class\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(test_predictions)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d77d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b5d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 51s 4s/step - loss: 1.4734 - accuracy: 0.4369 - val_loss: 0.4973 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.7671 - accuracy: 0.7138 - val_loss: 0.3367 - val_accuracy: 0.9125 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 36s 3s/step - loss: 0.5056 - accuracy: 0.8185 - val_loss: 0.2746 - val_accuracy: 0.9125 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.4724 - accuracy: 0.8462 - val_loss: 0.2298 - val_accuracy: 0.9250 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 35s 3s/step - loss: 0.3656 - accuracy: 0.8738 - val_loss: 0.1009 - val_accuracy: 0.9625 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.2959 - accuracy: 0.9108 - val_loss: 0.1898 - val_accuracy: 0.9375 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.2712 - accuracy: 0.8985 - val_loss: 0.1846 - val_accuracy: 0.9125 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 37s 3s/step - loss: 0.2293 - accuracy: 0.9262 - val_loss: 0.2207 - val_accuracy: 0.9250 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.2406 - accuracy: 0.9169 - val_loss: 0.1584 - val_accuracy: 0.9500 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.1846 - accuracy: 0.9385 - val_loss: 0.1283 - val_accuracy: 0.9500 - lr: 4.0657e-04\n",
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 6s 318ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: humanitarianaid\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)  # Changed to match DenseNet201 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,  # Increased rotation range\n",
    "    width_shift_range=0.3,  # Increased width shift range\n",
    "    height_shift_range=0.3,  # Increased height shift range\n",
    "    shear_range=0.3,  # Increased shear range\n",
    "    zoom_range=0.3,  # Increased zoom range\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"  # Updated path to the testing folder\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Changed to Global Average Pooling\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:  # Fine-tuning last few layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095fd9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 65s 5s/step - loss: 1.4902 - accuracy: 0.4187 - val_loss: 0.7569 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.7599 - accuracy: 0.7406 - val_loss: 0.4078 - val_accuracy: 0.8500 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.6702 - accuracy: 0.7437 - val_loss: 0.3074 - val_accuracy: 0.9000 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.5440 - accuracy: 0.8125 - val_loss: 0.3719 - val_accuracy: 0.8875 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.3964 - accuracy: 0.8656 - val_loss: 0.3229 - val_accuracy: 0.8625 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.3775 - accuracy: 0.8594 - val_loss: 0.2539 - val_accuracy: 0.9250 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.3117 - accuracy: 0.9062 - val_loss: 0.1676 - val_accuracy: 0.9500 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2601 - accuracy: 0.9250 - val_loss: 0.2928 - val_accuracy: 0.9000 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2610 - accuracy: 0.9187 - val_loss: 0.1882 - val_accuracy: 0.9250 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2520 - accuracy: 0.9000 - val_loss: 0.2606 - val_accuracy: 0.8875 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2360 - accuracy: 0.9094 - val_loss: 0.2630 - val_accuracy: 0.9000 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2185 - accuracy: 0.9156 - val_loss: 0.1581 - val_accuracy: 0.9375 - lr: 3.3287e-04\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.2383 - accuracy: 0.9375 - val_loss: 0.1878 - val_accuracy: 0.9250 - lr: 3.0119e-04\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.1774 - accuracy: 0.9375 - val_loss: 0.2024 - val_accuracy: 0.8875 - lr: 2.7253e-04\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.1330 - accuracy: 0.9594 - val_loss: 0.1521 - val_accuracy: 0.9125 - lr: 2.4660e-04\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.1652 - accuracy: 0.9469 - val_loss: 0.2329 - val_accuracy: 0.9250 - lr: 2.2313e-04\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1779 - accuracy: 0.9375 - val_loss: 0.1433 - val_accuracy: 0.9625 - lr: 2.0190e-04\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1918 - accuracy: 0.9312 - val_loss: 0.1791 - val_accuracy: 0.9250 - lr: 1.8268e-04\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1991 - accuracy: 0.9281 - val_loss: 0.2274 - val_accuracy: 0.9125 - lr: 1.6530e-04\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1072 - accuracy: 0.9719 - val_loss: 0.2315 - val_accuracy: 0.9125 - lr: 1.4957e-04\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<0,2,5>,3>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 110>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m    109\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    111\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Image\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Preprocess the image and make predictions\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<3,4,-1>,struct cv::impl::A0x11a46be7::Set<0,2,5>,3>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMOklEQVR4nO3bf6jd9X3H8edrSQOr7ao0t6VLIstGrGZDh95aKfthVzYT+0co+IdaJpNCEGrpn8pg7cB/1j8GpfgjBAnSf5p/Kl060srYaB0419yARqMod5HpbQRjLR1YmETf++OebmfXm9zvPTn3JH3zfMCF+/1+P+d831x5+r3nm+9NVSGpp9+42ANI2jgGLjVm4FJjBi41ZuBSYwYuNbZm4EkOJXkjyfPnOJ4k30qymOREkuunP6akSQy5gj8G7DnP8b3ArtHXfuCRCx9L0jSsGXhVPQm8dZ4l+4Bv17KngcuTfGJaA0qa3OYpvMc24LWx7aXRvtdXLkyyn+WrPJdddtkNV1999RROL/V2/PjxN6tqbpLXTiPwrLJv1edfq+ogcBBgfn6+FhYWpnB6qbck/znpa6dxF30J2DG2vR04PYX3lXSBphH4EeCu0d30m4BfVNX7fj2XNHtr/oqe5DvAzcDWJEvA14EPAFTVAeAocCuwCPwSuHujhpW0PmsGXlV3rHG8gC9PbSJJU+OTbFJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmODAk+yJ8lLSRaT3L/K8Y8k+X6SZ5OcTHL39EeVtF5rBp5kE/AQsBfYDdyRZPeKZV8GXqiq64Cbgb9PsmXKs0papyFX8BuBxao6VVXvAIeBfSvWFPDhJAE+BLwFnJ3qpJLWbUjg24DXxraXRvvGPQhcA5wGngO+WlXvrXyjJPuTLCRZOHPmzIQjSxpqSOBZZV+t2L4FeAb4beAPgQeT/Nb7XlR1sKrmq2p+bm5unaNKWq8hgS8BO8a2t7N8pR53N/B4LVsEXgGuns6IkiY1JPBjwK4kO0c3zm4HjqxY8yrwOYAkHwc+CZya5qCS1m/zWguq6mySe4EngE3Aoao6meSe0fEDwAPAY0meY/lX+vuq6s0NnFvSAGsGDlBVR4GjK/YdGPv+NPAX0x1N0oXySTapMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxobFHiSPUleSrKY5P5zrLk5yTNJTib58XTHlDSJzWstSLIJeAj4c2AJOJbkSFW9MLbmcuBhYE9VvZrkYxs0r6R1GHIFvxFYrKpTVfUOcBjYt2LNncDjVfUqQFW9Md0xJU1iSODbgNfGtpdG+8ZdBVyR5EdJjie5a7U3SrI/yUKShTNnzkw2saTBhgSeVfbViu3NwA3A54FbgL9JctX7XlR1sKrmq2p+bm5u3cNKWp81P4OzfMXeMba9HTi9ypo3q+pt4O0kTwLXAS9PZUpJExlyBT8G7EqyM8kW4HbgyIo1/wD8cZLNST4IfBp4cbqjSlqvNa/gVXU2yb3AE8Am4FBVnUxyz+j4gap6MckPgRPAe8CjVfX8Rg4uaW2pWvlxejbm5+drYWHhopxb+nWS5HhVzU/yWp9kkxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGhsUeJI9SV5Kspjk/vOs+1SSd5PcNr0RJU1qzcCTbAIeAvYCu4E7kuw+x7pvAE9Me0hJkxlyBb8RWKyqU1X1DnAY2LfKuq8A3wXemOJ8ki7AkMC3Aa+NbS+N9v2vJNuALwAHzvdGSfYnWUiycObMmfXOKmmdhgSeVfbViu1vAvdV1bvne6OqOlhV81U1Pzc3N3BESZPaPGDNErBjbHs7cHrFmnngcBKArcCtSc5W1femMaSkyQwJ/BiwK8lO4KfA7cCd4wuqauevvk/yGPCPxi1dfGsGXlVnk9zL8t3xTcChqjqZ5J7R8fN+7pZ08Qy5glNVR4GjK/atGnZV/dWFjyVpGnySTWrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgYFnmRPkpeSLCa5f5XjX0xyYvT1VJLrpj+qpPVaM/Akm4CHgL3AbuCOJLtXLHsF+NOquhZ4ADg47UElrd+QK/iNwGJVnaqqd4DDwL7xBVX1VFX9fLT5NLB9umNKmsSQwLcBr41tL432ncuXgB+sdiDJ/iQLSRbOnDkzfEpJExkSeFbZV6suTD7LcuD3rXa8qg5W1XxVzc/NzQ2fUtJENg9YswTsGNveDpxeuSjJtcCjwN6q+tl0xpN0IYZcwY8Bu5LsTLIFuB04Mr4gyZXA48BfVtXL0x9T0iTWvIJX1dkk9wJPAJuAQ1V1Msk9o+MHgK8BHwUeTgJwtqrmN25sSUOkatWP0xtufn6+FhYWLsq5pV8nSY5PesH0STapMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKmxQYEn2ZPkpSSLSe5f5XiSfGt0/ESS66c/qqT1WjPwJJuAh4C9wG7gjiS7VyzbC+wafe0HHpnynJImMOQKfiOwWFWnquod4DCwb8WafcC3a9nTwOVJPjHlWSWt0+YBa7YBr41tLwGfHrBmG/D6+KIk+1m+wgP8d5Ln1zXt7G0F3rzYQ5zHpT4fOOM0fHLSFw4JPKvsqwnWUFUHgYMASRaqan7A+S+aS33GS30+cMZpSLIw6WuH/Iq+BOwY294OnJ5gjaQZGxL4MWBXkp1JtgC3A0dWrDkC3DW6m34T8Iuqen3lG0marTV/Ra+qs0nuBZ4ANgGHqupkkntGxw8AR4FbgUXgl8DdA859cOKpZ+dSn/FSnw+ccRomni9V7/uoLKkJn2STGjNwqbEND/xSf8x1wHxfHM11IslTSa6b5XxDZhxb96kk7ya5bZbzjc695oxJbk7yTJKTSX58Kc2X5CNJvp/k2dF8Q+4jTXO+Q0neONezIRN3UlUb9sXyTbn/AH4X2AI8C+xeseZW4Acs/1v6TcC/b+RME8z3GeCK0fd7Zznf0BnH1v0Lyzc8b7vUZgQuB14Arhxtf+wSm++vgW+Mvp8D3gK2zHDGPwGuB54/x/GJOtnoK/il/pjrmvNV1VNV9fPR5tMs/xv/LA35GQJ8Bfgu8MYshxsZMuOdwONV9SpAVc1yziHzFfDhJAE+xHLgZ2c1YFU9OTrnuUzUyUYHfq5HWNe7ZqOs99xfYvn/orO05oxJtgFfAA7McK5xQ36OVwFXJPlRkuNJ7prZdMPmexC4huUHtJ4DvlpV781mvEEm6mTIo6oXYmqPuW6QwedO8lmWA/+jDZ1olVOvsm/ljN8E7quqd5cvQDM3ZMbNwA3A54DfBP4tydNV9fJGD8ew+W4BngH+DPg94J+S/GtV/dcGzzbURJ1sdOCX+mOug86d5FrgUWBvVf1sRrP9ypAZ54HDo7i3ArcmOVtV35vJhMP/O79ZVW8Dbyd5ErgOmEXgQ+a7G/i7Wv7Au5jkFeBq4CczmG+IyTrZ4BsHm4FTwE7+7+bG769Y83n+/82Dn8zwxsaQ+a5k+Qm9z8xqrvXOuGL9Y8z+JtuQn+M1wD+P1n4QeB74g0tovkeAvx19/3Hgp8DWGf8cf4dz32SbqJMNvYLXxj3mOsv5vgZ8FHh4dIU8WzP8y6OBM15UQ2asqheT/BA4AbwHPFpVM/lz4YE/wweAx5I8x3JE91XVzP6ENMl3gJuBrUmWgK8DHxibb6JOfFRVaswn2aTGDFxqzMClxgxcaszApcYMXGrMwKXG/gdHUAnhViIfbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (200, 200)  # Adjusted resolution\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255  # Rescale the pixel values to the range [0, 1]\n",
    ")\n",
    "\n",
    "# Only rescale for testing data\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (DenseNet201)\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(200, 200, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare a sample testing image for visualization\n",
    "test_img_path = r\"C:\\Users\\gupta\\Desktop\\Eyantra_GG(2023)\\Task_2B\\testing\\fire\\fire1.jpeg\"\n",
    "test_img = cv2.imread(test_img_path)\n",
    "test_img = cv2.resize(test_img, (200, 200))  # Resize the image to the model's input size\n",
    "test_img = test_img / 255.0  # Normalize pixel values\n",
    "\n",
    "# Visualize the original image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Preprocess the image and make predictions\n",
    "processed_img = test_img[np.newaxis, ...]\n",
    "predicted_probabilities = model.predict(processed_img)\n",
    "predicted_class = np.argmax(predicted_probabilities)\n",
    "predicted_label = class_names[predicted_class]\n",
    "\n",
    "# Visualize the processed image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Processed Image\\nPredicted Class: {predicted_label}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0fc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a47e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 80 images belonging to 5 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.2178 - accuracy: 0.5750 - val_loss: 1.2915 - val_accuracy: 0.6625 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 12s 1s/step - loss: 0.4851 - accuracy: 0.8469 - val_loss: 0.6756 - val_accuracy: 0.7750 - lr: 9.0484e-04\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.4234 - accuracy: 0.8562 - val_loss: 0.4668 - val_accuracy: 0.8625 - lr: 8.1873e-04\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2616 - accuracy: 0.8906 - val_loss: 1.0742 - val_accuracy: 0.7750 - lr: 7.4082e-04\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2826 - accuracy: 0.9125 - val_loss: 0.9105 - val_accuracy: 0.7750 - lr: 6.7032e-04\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2480 - accuracy: 0.9187 - val_loss: 1.0142 - val_accuracy: 0.8000 - lr: 6.0653e-04\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.2098 - accuracy: 0.9375 - val_loss: 0.4345 - val_accuracy: 0.8625 - lr: 5.4881e-04\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1900 - accuracy: 0.9469 - val_loss: 1.0685 - val_accuracy: 0.8125 - lr: 4.9659e-04\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1269 - accuracy: 0.9469 - val_loss: 1.1639 - val_accuracy: 0.8125 - lr: 4.4933e-04\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.1837 - accuracy: 0.9281 - val_loss: 1.3239 - val_accuracy: 0.7875 - lr: 4.0657e-04\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1692 - accuracy: 0.9406 - val_loss: 0.7588 - val_accuracy: 0.9000 - lr: 3.6788e-04\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.1216 - accuracy: 0.9594 - val_loss: 1.3025 - val_accuracy: 0.8500 - lr: 3.3287e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 5 classes.\n",
      "10/10 [==============================] - 1s 46ms/step\n",
      "Image: combat1.jpeg, Predicted Class: combat\n",
      "Image: combat2.jpeg, Predicted Class: combat\n",
      "Image: building1.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: building2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: fire1.jpeg, Predicted Class: fire\n",
      "Image: fire2.jpeg, Predicted Class: fire\n",
      "Image: rehab1.jpeg, Predicted Class: humanitarianaid\n",
      "Image: rehab2.jpeg, Predicted Class: destroyedbuilding\n",
      "Image: military1.jpeg, Predicted Class: militaryvehicles\n",
      "Image: military2.jpeg, Predicted Class: militaryvehicles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define class names and labels\n",
    "class_names = [\"combat\", \"destroyedbuilding\", \"fire\", \"humanitarianaid\", \"militaryvehicles\"]\n",
    "class_labels = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Define image and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data_dir = \"training\"\n",
    "test_data_dir = \"testing\"\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "# Build and compile the model (MobileNetV2)\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Training with a learning rate scheduler\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[lr_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"alien_attack_model.h5\")\n",
    "\n",
    "# Visualize training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [class_names[i] for i in np.argmax(test_predictions, axis=1)]\n",
    "\n",
    "# Display the predicted class labels for each test image\n",
    "for i, image_path in enumerate(test_generator.filepaths):\n",
    "    filename = os.path.basename(image_path)\n",
    "    class_name = predicted_labels[i]\n",
    "    print(f\"Image: {filename}, Predicted Class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af28f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd23623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 5 classes.\n",
      "400/400 [==============================] - 38s 90ms/step\n",
      "Classification Report (Training Data):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           Combat       1.00      1.00      1.00        80\n",
      "             Fire       1.00      1.00      1.00        80\n",
      "destroyedbuilding       1.00      1.00      1.00        80\n",
      "  humanitarianaid       1.00      1.00      1.00        80\n",
      " militaryvehicles       1.00      1.00      1.00        80\n",
      "\n",
      "         accuracy                           1.00       400\n",
      "        macro avg       1.00      1.00      1.00       400\n",
      "     weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "Confusion Matrix (Training Data):\n",
      "[[80  0  0  0  0]\n",
      " [ 0 80  0  0  0]\n",
      " [ 0  0 80  0  0]\n",
      " [ 0  0  0 80  0]\n",
      " [ 0  0  0  0 80]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 1\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"alien_attack_model.h5\")\n",
    "\n",
    "# Prepare the training data\n",
    "train_data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    \"training\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Make predictions on training data\n",
    "train_predictions = model.predict(train_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = [os.path.basename(os.path.dirname(path)) for path in train_generator.filepaths]\n",
    "\n",
    "# Get true labels\n",
    "true_labels = [os.path.basename(os.path.dirname(path)) for path in train_generator.filepaths]\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report (Training Data):\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "print(\"Confusion Matrix (Training Data):\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
